#!/usr/bin/env python3
"""
Job Processor Demo and Analysis Script
Shows the logic, functionality, and tests the job processor with real job data.
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, List

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def show_job_processor_logic():
    """
    Explain the Job Processor Logic - WHAT, WHEN, HOW
    """
    print("=" * 80)
    print("üîÑ JOB PROCESSOR LOGIC OVERVIEW")
    print("=" * 80)
    
    print("\nüéØ WHAT IT DOES:")
    print("‚Ä¢ Processes scraped job URLs to extract detailed job information")
    print("‚Ä¢ Analyzes job compatibility using AI (Llama3) and rule-based methods")
    print("‚Ä¢ Scores jobs based on user profile match (0.0 to 1.0 scale)")
    print("‚Ä¢ Updates database with detailed job analysis and compatibility scores")
    print("‚Ä¢ Provides intelligent job recommendations and filtering")
    
    print("\n‚è∞ WHEN IT'S USED:")
    print("‚Ä¢ After job scraping when URLs need to be processed into detailed job data")
    print("‚Ä¢ During automated job analysis workflows")
    print("‚Ä¢ When users want AI-powered job compatibility scoring")
    print("‚Ä¢ As part of the job application pipeline for intelligent filtering")
    
    print("\nüîß HOW IT WORKS (Multi-Stage Pipeline):")
    print("1. üì• JOB QUEUE MANAGEMENT")
    print("   - Loads scraped jobs from database (status='scraped')")
    print("   - Creates processing queue with job tasks")
    print("   - Manages worker threads for concurrent processing")
    
    print("2. üåê JOB DESCRIPTION SCRAPING")
    print("   - Uses Playwright browser automation")
    print("   - Extracts detailed job descriptions from URLs")
    print("   - Handles different job board formats (Eluta, Indeed, etc.)")
    print("   - Caches results to avoid re-scraping")
    
    print("3. ü§ñ AI-POWERED ANALYSIS (Primary)")
    print("   - Uses Llama3 7B model via Ollama")
    print("   - Analyzes job requirements vs user profile")
    print("   - Generates compatibility scores and recommendations")
    print("   - Identifies skill matches and gaps")
    
    print("4. üìä RULE-BASED ANALYSIS (Fallback)")
    print("   - Enhanced keyword matching and scoring")
    print("   - Experience level matching")
    print("   - Location and salary analysis")
    print("   - Ensures processing continues if AI fails")
    
    print("5. üíæ DATABASE UPDATES")
    print("   - Updates job status to 'processed'")
    print("   - Stores compatibility scores and analysis data")
    print("   - Adds extracted keywords and skills")
    print("   - Maintains processing metadata")
    
    print("\nüîç KEY FEATURES:")
    print("‚Ä¢ Fault-tolerant with automatic fallbacks")
    print("‚Ä¢ Real-time processing status and statistics")
    print("‚Ä¢ Concurrent processing with worker threads")
    print("‚Ä¢ Comprehensive error handling and retry logic")
    print("‚Ä¢ Integration with dashboard for monitoring")

def get_sample_jobs_from_database():
    """Get sample jobs from the database for testing."""
    try:
        from src.core.job_database import get_job_db
        
        db = get_job_db()
        all_jobs = db.get_all_jobs()
        
        # Get different types of jobs for demonstration
        scraped_jobs = [job for job in all_jobs if job.get('status') == 'scraped']
        processed_jobs = [job for job in all_jobs if job.get('status') == 'processed']
        
        print(f"üìä DATABASE STATUS:")
        print(f"‚Ä¢ Total jobs: {len(all_jobs)}")
        print(f"‚Ä¢ Scraped jobs (need processing): {len(scraped_jobs)}")
        print(f"‚Ä¢ Processed jobs: {len(processed_jobs)}")
        
        if scraped_jobs:
            print(f"\nüìã SAMPLE SCRAPED JOBS (Need Processing):")
            for i, job in enumerate(scraped_jobs[:3]):
                url = job.get('url', 'No URL')
                keyword = job.get('search_keyword', 'No keyword')
                print(f"  {i+1}. {keyword} - {url[:60]}...")
        
        if processed_jobs:
            print(f"\nüìã SAMPLE PROCESSED JOBS:")
            for i, job in enumerate(processed_jobs[:3]):
                title = job.get('title', 'No title')
                company = job.get('company', 'No company')
                score = job.get('match_score', 0)
                print(f"  {i+1}. {title} at {company} (Score: {score:.2f})")
        
        return {
            'all_jobs': all_jobs,
            'scraped_jobs': scraped_jobs,
            'processed_jobs': processed_jobs
        }
        
    except Exception as e:
        logger.warning(f"Could not access database: {e}")
        print("‚ö†Ô∏è  Database not accessible. Using sample data.")
        return create_sample_job_data()

def create_sample_job_data():
    """Create sample job data for testing."""
    return {
        'scraped_jobs': [
            {
                'id': 1,
                'url': 'https://eluta.ca/job/senior-python-developer-123',
                'status': 'scraped',
                'search_keyword': 'python developer',
                'title': 'Pending Processing',
                'company': 'Unknown',
                'location': 'Unknown'
            },
            {
                'id': 2,
                'url': 'https://eluta.ca/job/data-scientist-456',
                'status': 'scraped',
                'search_keyword': 'data scientist',
                'title': 'Pending Processing',
                'company': 'Unknown',
                'location': 'Unknown'
            }
        ],
        'processed_jobs': []
    }

def demonstrate_processing_pipeline():
    """Demonstrate the job processing pipeline."""
    print("\n" + "=" * 80)
    print("üîÑ DEMONSTRATING PROCESSING PIPELINE")
    print("=" * 80)
    
    print("\n1. üì• QUEUE MANAGEMENT:")
    print("   ‚Ä¢ JobProcessorQueue manages worker threads")
    print("   ‚Ä¢ JobTask objects represent individual jobs to process")
    print("   ‚Ä¢ Queue supports priority and retry mechanisms")
    
    print("\n2. üåê JOB DESCRIPTION SCRAPING:")
    print("   ‚Ä¢ EnhancedJobDescriptionScraper extracts job details")
    print("   ‚Ä¢ Uses Playwright for JavaScript-heavy sites")
    print("   ‚Ä¢ Handles different job board formats automatically")
    print("   ‚Ä¢ Caches results to improve performance")
    
    print("\n3. ü§ñ AI ANALYSIS PROCESS:")
    print("   ‚Ä¢ ReliableJobProcessorAnalyzer coordinates analysis")
    print("   ‚Ä¢ Checks Ollama connection before attempting AI")
    print("   ‚Ä¢ Uses Llama3 7B model for intelligent analysis")
    print("   ‚Ä¢ Falls back to rule-based analysis if AI fails")
    
    print("\n4. üìä ANALYSIS COMPONENTS:")
    print("   ‚Ä¢ Compatibility scoring (0.0 to 1.0)")
    print("   ‚Ä¢ Skill matching and gap analysis")
    print("   ‚Ä¢ Experience level assessment")
    print("   ‚Ä¢ Location and cultural fit evaluation")
    
    print("\n5. üíæ DATABASE INTEGRATION:")
    print("   ‚Ä¢ Updates job status from 'scraped' to 'processed'")
    print("   ‚Ä¢ Stores match_score for filtering and sorting")
    print("   ‚Ä¢ Saves full analysis_data as JSON")
    print("   ‚Ä¢ Maintains processing metadata and timestamps")

def test_job_processor_components():
    """Test individual job processor components."""
    print("\n" + "=" * 80)
    print("üß™ TESTING JOB PROCESSOR COMPONENTS")
    print("=" * 80)
    
    # Test 1: Database Connection
    print("\n1. üìä TESTING DATABASE CONNECTION:")
    try:
        from src.core.job_database import get_job_db
        db = get_job_db()
        job_count = db.get_job_count()
        print(f"   ‚úÖ Database connected - {job_count} jobs found")
    except Exception as e:
        print(f"   ‚ùå Database connection failed: {e}")
        return False
    
    # Test 2: AI Service Connection
    print("\n2. ü§ñ TESTING AI SERVICE CONNECTION:")
    try:
        from src.services.ollama_connection_checker import get_ollama_checker
        checker = get_ollama_checker()
        is_available = checker.is_available()
        if is_available:
            print("   ‚úÖ Ollama service is available")
            models = checker.get_available_models()
            print(f"   üìã Available models: {', '.join(models) if models else 'None'}")
        else:
            print("   ‚ö†Ô∏è  Ollama service not available - will use rule-based analysis")
    except Exception as e:
        print(f"   ‚ùå AI service check failed: {e}")
    
    # Test 3: Enhanced Job Processor
    print("\n3. üîÑ TESTING ENHANCED JOB PROCESSOR:")
    try:
        from src.dashboard.enhanced_job_processor import get_enhanced_job_processor
        processor = get_enhanced_job_processor("Nirajan")
        status = processor.get_status()
        print(f"   ‚úÖ Job processor initialized")
        print(f"   üìä Status: Active={status['active']}, Profile={status['profile']}")
    except Exception as e:
        print(f"   ‚ùå Job processor initialization failed: {e}")
        return False
    
    # Test 4: Rule-based Analyzer
    print("\n4. üìä TESTING RULE-BASED ANALYZER:")
    try:
        from src.ai.enhanced_rule_based_analyzer import EnhancedRuleBasedAnalyzer
        from src.utils.profile_helpers import load_profile
        
        profile = load_profile("Nirajan")
        analyzer = EnhancedRuleBasedAnalyzer(profile)
        
        test_job = {
            'title': 'Senior Python Developer',
            'description': 'Looking for Python developer with SQL experience',
            'location': 'Toronto, ON',
            'company': 'Test Company'
        }
        
        result = analyzer.analyze_job(test_job)
        score = result.get('compatibility_score', 0)
        print(f"   ‚úÖ Rule-based analysis working - Score: {score:.2f}")
        
    except Exception as e:
        print(f"   ‚ùå Rule-based analyzer test failed: {e}")
    
    return True

def run_job_processor_demo():
    """Run a live demonstration of the job processor."""
    print("\n" + "=" * 80)
    print("üöÄ RUNNING JOB PROCESSOR DEMO")
    print("=" * 80)
    
    # Get jobs from database
    job_data = get_sample_jobs_from_database()
    scraped_jobs = job_data.get('scraped_jobs', [])
    
    if not scraped_jobs:
        print("‚ùå No scraped jobs found to process")
        print("üí° Run the scraper first to get some job URLs")
        return False
    
    print(f"\nüì• Found {len(scraped_jobs)} scraped jobs to process")
    
    # Initialize processor
    try:
        from src.dashboard.enhanced_job_processor import get_enhanced_job_processor
        processor = get_enhanced_job_processor("Nirajan")
        
        print(f"‚úÖ Job processor initialized for profile: Nirajan")
        
        # Start processing
        print(f"\nüîÑ Starting job processor...")
        success = processor.start_processing()
        
        if not success:
            print("‚ùå Failed to start job processor")
            return False
        
        print("‚úÖ Job processor started successfully")
        
        # Add jobs to processing queue
        print(f"\nüì• Adding jobs to processing queue...")
        added_count = processor.add_jobs_for_processing(scraped_jobs[:3])  # Process first 3 jobs
        
        if added_count == 0:
            print("‚ùå No jobs were added to the processing queue")
            processor.stop_processing()
            return False
        
        print(f"‚úÖ Added {added_count} jobs to processing queue")
        
        # Monitor processing
        print(f"\n‚è±Ô∏è  Processing {added_count} jobs...")
        print("   (This may take a few minutes depending on AI service availability)")
        
        start_time = time.time()
        last_processed = 0
        
        for i in range(60):  # Monitor for up to 60 iterations (5 minutes)
            status = processor.get_status()
            processed_count = status['processed_count']
            queue_size = status['queue_size']
            error_count = status['error_count']
            ai_analyzed_count = status['ai_analyzed_count']
            
            # Show progress when it changes
            if processed_count != last_processed:
                elapsed = time.time() - start_time
                print(f"   Progress: {processed_count}/{added_count} | AI: {ai_analyzed_count} | Errors: {error_count} | Time: {elapsed:.1f}s")
                last_processed = processed_count
            
            # Check if processing is complete
            if queue_size == 0 and processed_count >= added_count:
                print("   ‚úÖ Processing complete!")
                break
            
            time.sleep(5)  # Check every 5 seconds
        
        # Get final results
        final_status = processor.get_status()
        stats = final_status['stats']
        
        print(f"\nüìà PROCESSING RESULTS:")
        print(f"   Total processed: {final_status['processed_count']}")
        print(f"   AI analyzed: {final_status['ai_analyzed_count']}")
        print(f"   Errors: {final_status['error_count']}")
        print(f"   Average AI score: {stats['average_ai_score']:.2f}")
        print(f"   High matches (‚â•0.8): {stats['high_matches_found']}")
        
        # Show analysis method breakdown
        methods = stats['analysis_methods']
        print(f"\nüîç ANALYSIS METHODS USED:")
        print(f"   AI: {methods['ai']}")
        print(f"   Enhanced Rule-based: {methods['enhanced_rule_based']}")
        print(f"   Fallback: {methods['fallback']}")
        
        # Show AI service health
        ai_health = stats['ai_service_health']
        print(f"\nü§ñ AI SERVICE HEALTH:")
        print(f"   Connection status: {ai_health['connection_status']}")
        print(f"   Consecutive failures: {ai_health['consecutive_failures']}")
        print(f"   Last successful AI: {ai_health['last_successful_ai']}")
        
        # Stop processor
        print(f"\n‚èπÔ∏è Stopping job processor...")
        processor.stop_processing()
        
        # Check database after processing
        from src.core.job_database import get_job_db
        db = get_job_db()
        updated_jobs = db.get_all_jobs()
        processed_jobs = [job for job in updated_jobs if job.get('status') == 'processed']
        
        print(f"\nüìä DATABASE AFTER PROCESSING:")
        print(f"   Total jobs: {len(updated_jobs)}")
        print(f"   Processed jobs: {len(processed_jobs)}")
        
        # Show sample processed jobs
        recent_processed = [job for job in processed_jobs if job.get('match_score', 0) > 0][-3:]
        if recent_processed:
            print(f"\nüìã RECENTLY PROCESSED JOBS:")
            for i, job in enumerate(recent_processed):
                title = job.get('title', 'No title')
                company = job.get('company', 'No company')
                score = job.get('match_score', 0)
                print(f"   {i+1}. {title} at {company} (Score: {score:.2f})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        logger.error(f"Job processor demo failed: {e}")
        return False

def show_processing_architecture():
    """Show the job processor architecture."""
    print("\n" + "=" * 80)
    print("üèóÔ∏è JOB PROCESSOR ARCHITECTURE")
    print("=" * 80)
    
    print("\nüìä COMPONENT HIERARCHY:")
    print("‚îå‚îÄ EnhancedJobProcessor (Main Controller)")
    print("‚îú‚îÄ‚îÄ‚îÄ JobProcessorQueue (Queue Management)")
    print("‚îú‚îÄ‚îÄ‚îÄ ReliableJobProcessorAnalyzer (AI Coordination)")
    print("‚îÇ    ‚îú‚îÄ‚îÄ‚îÄ EnhancedJobAnalyzer (AI Analysis)")
    print("‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ EnhancedRuleBasedAnalyzer (Fallback)")
    print("‚îú‚îÄ‚îÄ‚îÄ EnhancedJobDescriptionScraper (Web Scraping)")
    print("‚îú‚îÄ‚îÄ‚îÄ OllamaConnectionChecker (AI Health)")
    print("‚îî‚îÄ‚îÄ‚îÄ ModernJobDatabase (Data Storage)")
    
    print("\nüîÑ DATA FLOW:")
    print("1. Scraped URLs (status='scraped') ‚Üí Processing Queue")
    print("2. Queue ‚Üí Worker Threads ‚Üí Job Description Scraping")
    print("3. Job Details ‚Üí AI Analysis (Llama3) or Rule-based")
    print("4. Analysis Results ‚Üí Database Update (status='processed')")
    print("5. Processed Jobs ‚Üí Dashboard Display & Filtering")
    
    print("\n‚ö° FAULT TOLERANCE:")
    print("‚Ä¢ AI Service Down ‚Üí Automatic fallback to rule-based analysis")
    print("‚Ä¢ Network Issues ‚Üí Retry mechanism with exponential backoff")
    print("‚Ä¢ Scraping Failures ‚Üí Error logging and job marking")
    print("‚Ä¢ Database Errors ‚Üí Transaction rollback and error recovery")
    print("‚Ä¢ Worker Crashes ‚Üí Automatic worker restart and task requeue")

def main():
    """Main function to run the job processor demo."""
    print("üöÄ JOB PROCESSOR DEMO STARTING...")
    
    # Show the logic overview
    show_job_processor_logic()
    
    # Demonstrate processing pipeline
    demonstrate_processing_pipeline()
    
    # Show architecture
    show_processing_architecture()
    
    # Test components
    components_ok = test_job_processor_components()
    
    if not components_ok:
        print("\n‚ùå Component tests failed - skipping live demo")
        print("üí° Check your Ollama installation and database setup")
        return
    
    # Run live demo
    demo_success = run_job_processor_demo()
    
    print("\n" + "=" * 80)
    print("üìä DEMO SUMMARY")
    print("=" * 80)
    
    if demo_success:
        print("‚úÖ Job processor demo completed successfully!")
        print("‚úÖ All processing mechanisms working properly")
        print("‚úÖ Jobs processed and scored in database")
    else:
        print("‚ùå Some parts of the demo failed - check logs for details")
    
    print("\nüéØ KEY TAKEAWAYS:")
    print("‚Ä¢ Job processor uses multi-stage pipeline for reliability")
    print("‚Ä¢ AI analysis provides intelligent compatibility scoring")
    print("‚Ä¢ Rule-based fallback ensures processing continues")
    print("‚Ä¢ System handles errors gracefully with comprehensive logging")
    print("‚Ä¢ Real-time monitoring and statistics available")
    print("‚Ä¢ Processed jobs can be filtered and sorted by compatibility score")
    
    print("\nüîó NEXT STEPS:")
    print("‚Ä¢ Check the dashboard to see processed jobs")
    print("‚Ä¢ Use compatibility scores to filter high-match jobs")
    print("‚Ä¢ Generate documents for top-scoring positions")
    print("‚Ä¢ Set up automated application workflows")

if __name__ == "__main__":
    main()